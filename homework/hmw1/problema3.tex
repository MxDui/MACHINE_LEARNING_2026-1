\section*{3. [2.5 puntos] Función de pérdida esperada}

Dados los valores de una función de pérdida $L_{kj}$, el riesgo esperado

\begin{equation}
E[L] = \sum_{k} \sum_{j} \int_{R_j} L_{kj} p(x, \mathcal{C}_k) dx
\end{equation}

se minimiza si para cada $x$ escogemos la clase para minimizar la siguiente expresión (ya que $p(x)$ no contiene al parámetro de clase):

\begin{equation}
\arg\min_{j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k|x) p(x) \right) = \arg\min_{j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k|x) \right)
\end{equation}

Verifica que si $L_{kj} = 1 - I_{kj}$, donde $I_{kj}$ son los elementos de la matriz identidad, entonces la minimización anterior se reduce a escoger la clase $i$ con la probabilidad más grande $p(\mathcal{C}_i|x)$.

\textbf{Solución:}

\textbf{Desarrollo de la demostración:}

Analizo un problema de decisión bayesiana donde:
\begin{itemize}
    \item $\mathcal{C}_k$ representa las clases verdaderas ($k = 1, 2, \ldots, K$)
    \item $j$ representa las decisiones o predicciones ($j = 1, 2, \ldots, K$)
    \item $L_{kj}$ es la pérdida incurrida al decidir la clase $j$ cuando la clase verdadera es $k$
    \item $p(\mathcal{C}_k|x)$ es la probabilidad a posteriori de la clase $k$ dado el input $x$
\end{itemize}

\textbf{Definición de la función de pérdida 0-1:}

La función de pérdida $L_{kj} = 1 - I_{kj}$ donde $I_{kj}$ es el elemento $(k,j)$ de la matriz identidad se expresa como:

$$L_{kj} = 1 - I_{kj} = \begin{cases}
0 & \text{si } k = j \text{ (clasificación correcta)} \\
1 & \text{si } k \neq j \text{ (clasificación incorrecta)}
\end{cases}$$

Esta es la función de pérdida 0-1 estándar que penaliza igualmente todos los errores de clasificación.

\textbf{Calcular el riesgo esperado para una decisión específica}

Para una decisión fija $j$, el riesgo esperado es:
$$R(j|x) = \sum_{k} L_{kj} p(\mathcal{C}_k|x)$$

Sustituyendo $L_{kj} = 1 - I_{kj}$:

\begin{align}
R(j|x) &= \sum_{k} (1 - I_{kj}) p(\mathcal{C}_k|x) \\
&= \sum_{k} p(\mathcal{C}_k|x) - \sum_{k} I_{kj} p(\mathcal{C}_k|x)
\end{align}

\textbf{Simplificar usando propiedades de las probabilidades}

Analizamos cada término:

\textbf{Primer término:} $\sum_{k} p(\mathcal{C}_k|x) = 1$ (axioma de probabilidad)

\textbf{Segundo término:} $\sum_{k} I_{kj} p(\mathcal{C}_k|x)$

Por definición de la matriz identidad, $I_{kj} = 1$ solo cuando $k = j$, y $I_{kj} = 0$ en todos los demás casos. Por tanto:

$$\sum_{k} I_{kj} p(\mathcal{C}_k|x) = I_{jj} p(\mathcal{C}_j|x) + \sum_{k \neq j} I_{kj} p(\mathcal{C}_k|x) = 1 \cdot p(\mathcal{C}_j|x) + \sum_{k \neq j} 0 \cdot p(\mathcal{C}_k|x) = p(\mathcal{C}_j|x)$$

\textbf{Expresión final del riesgo}

Combinando los resultados:
$$R(j|x) = 1 - p(\mathcal{C}_j|x)$$

\textbf{Interpretación:} El riesgo de decidir la clase $j$ es igual a la probabilidad de que la clase verdadera NO sea $j$.

\textbf{Encontrar la decisión óptima}

Para minimizar el riesgo esperado, debemos encontrar:
$$j^* = \arg\min_{j} R(j|x) = \arg\min_{j} [1 - p(\mathcal{C}_j|x)]$$

Como el término constante $1$ no afecta la minimización:
$$j^* = \arg\min_{j} [-p(\mathcal{C}_j|x)] = \arg\max_{j} p(\mathcal{C}_j|x)$$

\textbf{Conclusión y interpretación}

\textbf{Resultado principal:} Con la función de pérdida 0-1, la regla de decisión óptima de Bayes es:

$$i^* = \arg\max_{j} p(\mathcal{C}_j|x)$$

\textbf{Interpretación práctica:} 
\begin{itemize}
    \item Elegir siempre la clase con mayor probabilidad a posteriori
    \item Esta regla minimiza la probabilidad de error de clasificación
    \item Es la base teórica para muchos clasificadores bayesianos
\end{itemize}

\textbf{Verificación matemática}

Para verificar que efectivamente hemos demostrado lo requerido, observemos que:

\begin{enumerate}
    \item \textbf{Partimos de:} $\arg\min_{j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k|x) \right)$ con $L_{kj} = 1 - I_{kj}$
    
    \item \textbf{Demostramos que esto es igual a:} $\arg\min_{j} [1 - p(\mathcal{C}_j|x)]$
    
    \item \textbf{Lo cual es equivalente a:} $\arg\max_{j} p(\mathcal{C}_j|x)$
    
    \item \textbf{Por tanto:} La minimización se reduce efectivamente a escoger la clase $i$ con la probabilidad más grande $p(\mathcal{C}_i|x)$ $\blacksquare$
\end{enumerate}

\textbf{Extensión teórica:}

Este resultado es fundamental en la teoría de decisiones bayesianas porque establece que, bajo pérdida 0-1:
\begin{itemize}
    \item La frontera de decisión óptima está dada por $p(\mathcal{C}_i|x) = p(\mathcal{C}_j|x)$ para clases $i \neq j$
    \item El error de Bayes (error mínimo alcanzable) es $1 - \max_j p(\mathcal{C}_j|x)$
    \item Cualquier clasificador que no siga esta regla tendrá error subóptimo
\end{itemize}