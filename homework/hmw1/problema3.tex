\section*{3. [2.5 puntos] Función de pérdida esperada}

Dados los valores de una función de pérdida $L_{kj}$, el riesgo esperado

\begin{equation}
E[L] = \sum_{k} \sum_{j} \int_{R_j} L_{kj} p(x, \mathcal{C}_k) dx
\end{equation}

se minimiza si para cada $x$ escogemos la clase para minimizar la siguiente expresión (ya que $p(x)$ no contiene al parámetro de clase):

\begin{equation}
\arg\min_{j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k|x) p(x) \right) = \arg\min_{j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k|x) \right)
\end{equation}

Verifica que si $L_{kj} = 1 - I_{kj}$, donde $I_{kj}$ son los elementos de la matriz identidad, entonces la minimización anterior se reduce a escoger la clase $i$ con la probabilidad más grande $p(\mathcal{C}_i|x)$.

\textbf{Demostración}


\textbf{Definición de la función de pérdida 0-1}

La función de pérdida $L_{kj} = 1 - I_{kj}$ donde $I_{kj}$ es el elemento $(k,j)$ de la matriz identidad se expresa como:

$$L_{kj} = 1 - I_{kj} = \begin{cases}
0 & \text{si } k = j \text{ (clasificación correcta)} \\
1 & \text{si } k \neq j \text{ (clasificación incorrecta)}
\end{cases}$$

Esta es la función de pérdida 0-1 estándar que penaliza igualmente todos los errores de clasificación.


Para una decisión fija $j$, el riesgo esperado es:
$$R(j|x) = \sum_{k} L_{kj} p(\mathcal{C}_k|x)$$

Sustituyendo $L_{kj} = 1 - I_{kj}$:

\begin{align}
R(j|x) &= \sum_{k} (1 - I_{kj}) p(\mathcal{C}_k|x) \\
&= \sum_{k} p(\mathcal{C}_k|x) - \sum_{k} I_{kj} p(\mathcal{C}_k|x)
\end{align}

\textbf{Simplificamos}

\textbf{Primer término:} $\sum_{k} p(\mathcal{C}_k|x) = 1$ (axioma de probabilidad)

\textbf{Segundo término:} $\sum_{k} I_{kj} p(\mathcal{C}_k|x)$

Por definición de la matriz identidad, $I_{kj} = 1$ solo cuando $k = j$, y $I_{kj} = 0$ en todos los demás casos. Por tanto:

$$\sum_{k} I_{kj} p(\mathcal{C}_k|x) = I_{jj} p(\mathcal{C}_j|x) + \sum_{k \neq j} I_{kj} p(\mathcal{C}_k|x) = 1 \cdot p(\mathcal{C}_j|x) + \sum_{k \neq j} 0 \cdot p(\mathcal{C}_k|x) = p(\mathcal{C}_j|x)$$


Combinando los resultados:
$$R(j|x) = 1 - p(\mathcal{C}_j|x)$$

\textbf{Interpretación} El riesgo de decidir la clase $j$ es igual a la probabilidad de que la clase verdadera NO sea $j$.


Para minimizar el riesgo esperado, debemos encontrar:
$$j^* = \arg\min_{j} R(j|x) = \arg\min_{j} [1 - p(\mathcal{C}_j|x)]$$

Como el término constante $1$ no afecta la minimización:
$$j^* = \arg\min_{j} [-p(\mathcal{C}_j|x)] = \arg\max_{j} p(\mathcal{C}_j|x)$$

\textbf{Conclusión}

Con la función de pérdida 0-1, la regla de decisión óptima de Bayes es:

$$i^* = \arg\max_{j} p(\mathcal{C}_j|x)$$

